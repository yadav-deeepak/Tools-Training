######################
‚ùÑÔ∏è What is Docker ?
#####################
‚û§ Docker is a free and open-source containerization software that allows you to package applications along with their dependencies into a single unit called a Docker Image. This image can run on any system that has Docker installed, making deployment easy and consistent across different environments.

üåü Why use Docker?
*******************
‚úÖ Portability ‚Äì Run the same application on any machine, regardless of OS configuration.
‚úÖ Dependency Management ‚Äì Ensures that all required software (e.g., libraries, databases, and runtimes) is included within the image.
‚úÖ Fast Deployment ‚Äì No need to manually install dependencies every time you set up a new environment.
‚úÖ Resource Efficiency ‚Äì Uses fewer resources compared to traditional virtual machines.

######################
‚ùÑÔ∏è How Docker Works?
######################
‚û§ Create a Docker Image ‚Äì Package the app code + dependencies into a lightweight container image.
‚û§ Run the Docker Container ‚Äì Deploy this image as a container using the docker run command.
‚û§ Execute Anywhere ‚Äì The same image runs on any machine with Docker installed.
‚û§ Once the image is built, it can run on any machine without requiring additional software setup.

#########################
‚ùÑÔ∏è What is kubernetes ?
#########################
‚û§ Kubernetes (K8s) is a free and open source orchestration(management) tool developed by google to manage containerized applications. 
‚û§ orchestration = managing multiple containers efficiently.

‚û§ Kubernetes automates key tasks like: 
1. Creating , Starting and stopping containers 
2. Scaling up/down based on demand 
3. Handling failures automatically. 

#########################
‚ùÑÔ∏è Why use kubernetes ? 
#########################
‚úÖ Orchestration ‚Äì Efficiently manages multiple containers across a cluster of machines.
‚úÖ Self-Healing ‚Äì If a container crashes, Kubernetes automatically replaces it.
‚úÖ Load Balancing ‚Äì Distributes traffic across multiple containers to avoid overloading.
‚úÖ Auto Scaling ‚Äì Increases or decreases the number of running containers based on traffic load.
‚úÖ Automated Deployments ‚Äì Supports CI/CD for rolling updates and version control.

üåü Kubernetes Advantages 
*************************
1. Orchestration : Managing containers 
---------------------------------------
‚û§ Kubernetes helps manage multiple docker containers across different machines (nodes) efficiently 
‚û§ Instead of running docker run manually for each container, kubernetes automates deployment 
‚û§ It ensures that all containers are running smoothly and adjusts their status as needed.

üëâ Note: Kubernetes ensures all these containers are running ,Healthy , and adjusts their status as needed.

2. Self healing : Automatic recovery 
--------------------------------------
‚û§ If a container crashes due to an error or system failure ,Kubernetes automatically restarts a new instance.
Ex: A web server container(Apache, Nginx) stops unexpectedly.
‚û§ Kubernetes detects the failure and starts a new container to replace it.
‚û§ Users never notice downtime.

3. Load Balancing ‚Äì Distributes Traffic Efficiently
---------------------------------------------------
‚û§ Kubernetes distributes incoming user requests across multiple containers to avoid overloading any single instance.

üìå Example:
------------
A shopping website experiences high traffic during a sale.

‚û§ Kubernetes ensures that requests are evenly distributed across available backend servers.
‚û§ Prevents server crashes and ensures smooth performance.

4. Auto Scaling ‚Äì Adjusting Resources Dynamically
--------------------------------------------------
‚û§ Kubernetes can increase or decrease the number of containers automatically based on traffic load.

üìå Example:
‚û§ If website traffic increases, Kubernetes adds more containers to handle the load.
‚û§ If traffic reduces, Kubernetes removes extra containers to save resources.
‚û§ Works similarly to cloud-based Auto Scaling Groups (ASG).

‚û§ Conclusion
================
üöÄ Docker simplifies packaging applications into portable containers.
üöÄ Kubernetes ensures these containers are orchestrated, scalable, and reliable.

üëâ Together, Docker & Kubernetes enable modern cloud-native application deployment‚Äîmaking applications highly available, efficient, and automated.

########################################################
‚ùÑÔ∏è Kubernetes (K8s) Architecture - Explained in Detail
#########################################################
1) Control Plane (Master Node/Control Node)
**********************************************
-> The control plane is responsible for managing the Kubernetes cluster. It includes the following components:

1. API Server: Receives requests from kubectl and manages cluster operations.

2. Scheduler: Identifies pending tasks in ETCD and assigns them to worker nodes.

3. Controller Manager: Ensures the cluster‚Äôs desired state matches the actual state.

4. ETCD: A distributed key-value store that acts as Kubernetes' internal database.

2) Worker Nodes (Slave Nodes)
******************************
-> Worker nodes run application workloads. They include the following components:

1. Kubelet: A node agent that communicates with the control plane and manages containers.

2. Kube Proxy: Manages networking and ensures communication within the cluster.

3. Docker Engine: Runs and manages containerized applications.

4. Pod: The smallest deployable unit in Kubernetes, housing one or more containers.

5. Container: Runs inside a Pod and contains the application code.

####################################
‚ùÑÔ∏è Explanation k8S working
####################################
‚û§ Step 1: To deploy an application, we interact with the control plane using the kubectl CLI.

‚û§ Step 2: The API Server receives the request and stores it in ETCD with a pending status.

‚û§ Step 3: The Scheduler identifies an available worker node to execute the task, using Kubelet for node management.

‚û§ Step 4: The Kubelet ensures the worker node is running the assigned workload.

‚û§ Step 5: The Kube Proxy manages networking for seamless cluster communication.

‚û§ Step 6: The Controller Manager continuously monitors the cluster to ensure tasks run correctly.

##########
Note:
##########
-> A cluster in Kubernetes (K8s) refers to a group of servers (nodes) that work together to run containerized applications. It consists of:
a. Control Plane (Master Node) ‚Äì Manages and controls the cluster.
b. Worker Nodes (Slave Nodes) ‚Äì Run application workloads inside containers.

###################################
‚ùÑÔ∏è Kubernetes (K8s) Cluster Setup
####################################
‚û§ A Kubernetes Cluster is a group of servers working together to run containerized applications. It can be set up in two main ways:
‚û§ A Kubernetes Cluster = Control Plane + Worker Nodes + Pods + Resources + Networking + Storage

1) Self-Managed Cluster
************************* 
In this setup, we manually install and manage Kubernetes on our own infrastructure.

a) MiniKube (Single Node)
==========================
-> Runs a single-node cluster on a local machine.
-> Best for learning and practicing Kubernetes concepts.
-> Not suitable for production as it lacks high availability and scalability.

b) Kubeadm (Multi-Node)
========================
-> A tool for setting up a multi-node cluster manually.
-> Requires configuring the control plane, worker nodes, and networking.
-> Gives full control over the cluster but requires deep Kubernetes expertise.
-> Used for on-premise or customized Kubernetes deployments.

2) Cloud Provider-Managed Cluster (Pre-configured, ready-to-use)
*******************************************************************
Cloud providers offer fully managed Kubernetes services, where they handle cluster maintenance, updates, and availability.

a) AWS EKS (Elastic Kubernetes Service)
=========================================
-> A managed Kubernetes service on Amazon Web Services.

b) Azure AKS (Azure Kubernetes Service)
========================================
-> Microsoft Azure‚Äôs managed Kubernetes offering.

c) GCP GKE (Google Kubernetes Engine)
======================================
-> Google Cloud‚Äôs fully managed Kubernetes solution.

#########################
‚ùÑÔ∏è MiniKube Setup
#######################
‚û§ Step-1 : Setup Linux VM
----------------------------
Login into AWS Cloud account
Create Linux VM with Ubuntu AMI - t2.medium
Select Storage as 50 GB or more with 2 vCPU required minimum(Default is 8 GB only for Linux)
Create Linux VM and connect to it using SSH Client

‚û§ Step-2 : Install Docker In Ubuntu VM
-----------------------------------------
sudo apt update
curl -fsSL get.docker.com | /bin/bash
sudo usermod -aG docker ubuntu 
exit

‚û§ Step-3 : Updating system packages before installing Minikube dependencies
------------------------------------------------------------------------------
sudo apt update
sudo apt install -y curl wget apt-transport-https

‚û§ Step-4 : Installing Minikube
--------------------------------
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
minikube version

// curl will send an http request and will get response.

‚û§ Step-5 : Install Kubectl (Kubernetes Client)
------------------------------------------------
-> We need to install this because instructions to the control plane is given by kubectl.
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/
kubectl version -o yaml

‚û§ Step-6 : Start MiniKube Server
-----------------------------------
minikube start ‚Äî driver=docker

‚û§ Step-7 : Check MiniKube status
-----------------------------------
minikube status

‚û§ Step-8 : Access K8S Cluster
--------------------------------
kubectl cluster-info

‚û§ Step-9 : Access K8S Nodes
-----------------------------
kubectl get nodes

#####################################
üöÄ What is a POD in Kubernetes?
#####################################

‚úÖ Key Concepts Explained:
-----------------------------
1. If you deploy an app, it will ultimately run inside one or more Pods. It is a building block to run app that we deploy in K8S

2. "Applications will be deployed as PODS in k8s."
Your app (e.g., a Spring Boot API) will be containerized using Docker. This container will then be wrapped inside a Pod and deployed on the cluster.

3. "To create PODS we will use Docker images."
A Pod runs one or more containers (usually one), and each container uses a Docker image. You can build a Docker image of your app and then deploy it inside a Pod.

4. "To create PODS we will use Manifest YML file."
A YAML manifest file defines the configuration for the Pod (or other resources like Deployments).

It includes:
a. The name of the Pod
b. The image to use
c. Ports to expose
c. Environment variables, etc.

5. "Create multiple PODS."
The same image (e.g., myapp:latest) can be used to create many Pods. This is how you scale your application‚Äîrunning multiple copies (Pods) to handle more traffic.

6. "If we run application with multiple pods then Load Balancing can be performed resulting in 99.9% uptime of the application."
High Availability: If one Pod crashes, others are still running, so your app stays available.Load Balancing: Kubernetes distributes traffic across Pods using a Service (like a load balancer).

7. "PODS count will be increased and decreased based on the demand (scalability)."
Kubernetes supports auto-scaling. You can scale Pods manually or automatically using a Horizontal Pod Autoscaler (HPA). Based on CPU/memory usage or custom metrics, Kubernetes will increase/decrease the number of Pods.


###############################
üöÄ Kubernetes Services
###############################

-> A Kubernetes Service is used to expose a group of Pods so that they can be accessed reliably. Since Pods can be created and destroyed at any time (with changing IPs), a Service gives them a stable network identity.

---------------------------
üß≠ Why Do We Use Services?
---------------------------

-> Pods are short-lived and can crash or restart.

-> Each time a Pod is created, it gets a new IP address.

-> Directly accessing Pods via IP is not reliable.

-> A Service gives a static IP to a group of Pods.

---------------------------------
üåê Types of Kubernetes Services
---------------------------------

Kubernetes offers different types of services depending on how and where you want to expose your Pods:

üîπ 1. ClusterIP (Default)
üî∏ 2. NodePort
üîπ 3. LoadBalancer

-------------------------------------------------------
üîê ClusterIP Service (Internal Access Only)
-------------------------------------------------------

üìå Key Points:
-> Pods are short-lived objects; if one crashes, Kubernetes replaces it with a new Pod.
-> Every new Pod gets a different IP address.

Note: üõë Never rely on Pod IPs to access an application.

-> A ClusterIP Service groups multiple Pods  and assigns them a single static IP.

-> This static IP allows other components inside the cluster to access the group of Pods reliably, even when individual Pods change.

---------------------
üö´ Access Scope:
---------------------

-> Only accessible within the Kubernetes cluster.

-> Not reachable from the outside world (internet or external clients).

----------------
üí° Use Case:
----------------
-> Internal services such as databases, backend APIs, authentication services, etc.

Example: You don‚Äôt want to expose a database Pod to the internet, so you use a ClusterIP service to allow access only from other internal Pods.

----------------------------------------------------
üåê What is a NodePort Service in Kubernetes?
----------------------------------------------------
-> A NodePort service is a type of Kubernetes Service that exposes your Pods outside the cluster using a port on each worker node (called a "NodePort").

üß≠ Why Use NodePort?
------------------------------------
‚û§ By default, Pods and ClusterIP services are only accessible within the cluster.
‚û§ NodePort makes them accessible externally by opening a static port (from 30000 to 32767) on each worker node.
‚û§ It allows you to access your application using:
http://<NodeIP>:<NodePort>

üëâ Note: Here all traffic is routed to one worker node. Means load balancing cannot happen here.

-----------------------------------------------------
üåê What is a LoadBalancer Service in Kubernetes?
-----------------------------------------------------
-> It not only provides external access to your app but also handles automatic traffic distribution across the backend Pods running on different worker nodes.

##############################################
üìÑ What is a Kubernetes Manifest YAML?
##############################################
-> Think of it as an instruction manual for Kubernetes to create and manage resources.

üß± Main Sections of a Manifest YAML
-----------------------------------
Here are the 4 main parts:

apiVersion: <version-number>   # API version to use
kind: <resource-type>          # Type of resource (Pod, Service, Deployment, etc.)
metadata:                      # Metadata like name, labels
spec:                          # Specification of what the resource should do

-------------------------------
üß™ Example: Pod Manifest YAML
Let‚Äôs look at a simple Pod definition:
-------------------------------

---
apiVersion: v1
kind: Pod
metadata:
  name: testpod
  labels:
    app: dempapp
spec:
  containers:
    - name: test
      image: psait/pankajsiracademy:latest
      ports:
        - containerPort: 9090
...

-------------------
üìÖ Explanation
-------------------
apiVersion: v1
‚û§ Tells Kubernetes to use version v1 of the API.
‚û§ Since you are creating a Pod, this is the correct API version.

kind: Pod
‚û§ Defines the type of resource you want to create.
‚û§ In this case, it‚Äôs a Pod, which is the smallest and simplest unit in Kubernetes.

metadata:
‚û§ Metadata gives Kubernetes basic info about your Pod.

name: testpod
‚û§ This is the name of your Pod.
‚û§ You‚Äôll use this name to check logs or status (e.g., kubectl get pod testpod).

labels:
‚û§ Labels are key-value pairs to categorize and group Kubernetes objects.
‚û§ app: dempapp is a label to help identify this Pod as part of the "dempapp" application.

spec:
‚û§ This section defines what‚Äôs inside the Pod.

containers:
‚û§ A Pod can contain one or more containers. You're defining one container here.

- name: test
‚û§ This is the name of the container inside the Pod (not the Pod itself).

image: psait/pankajsiracademy:latest
‚û§ This is the Docker image used to create the container.
‚û§ It will pull the latest version of psait/pankajsiracademy from Docker Hub or another registry.

‚ö†Ô∏è Make sure the image exists and is accessible (public or with correct credentials).

ports:
‚û§ This tells Kubernetes the container is listening on port 8080 inside the Pod.

containerPort: 8080
‚û§ This is the internal port your application is running on.
‚û§ Kubernetes can use this for things like service routing, health checks, etc.

#############
üåü Commands:
#############
Note: Save above content in .yml file

# execute manifest yml
kubectl apply -f <manifest-yml-file>

# check pods
kubectl get pods

# check pod logs
kubectl logs <pod-name>

# Describe pods
kubectl describe pod <pod-name>

# get pod logs
kubectl logs <pod-name>

---------------------------------------------
K8s Service Manifest YAML (for your Pod)
------------------------------------------

---
apiVersion: v1
kind: Service
metadata:
  name: testpod-service
spec:
  type: NodePort
  selector:
    app: dempapp           # This must match the Pod's label
  ports:
    - port: 80             # Exposed port for external access
      targetPort: 9090     # Port on which the app is running inside the container
      nodePort: 30080      # External port exposed on each node


üí° Explanation:
----------------------
name: testpod-service ‚Äì The name of the service.

type: NodePort ‚Äì Exposes the Pod outside the cluster.

selector.app: dempapp ‚Äì This matches the label of your Pod, so the service knows which Pod(s) to route to.

port: 80 ‚Äì The port used when calling the service.Port 80 is the default port for HTTP traffic

You're using a web server like Nginx, Apache, or similar.

targetPort: 8080 ‚Äì The port your container app actually listens on.

nodePort: 30080 ‚Äì External port accessible via http://<NodeIP>:30080

Commands
___________

# üîç Check existing services
kubectl get svc

# üì¶ Create the service using the YAML
kubectl apply -f testpod-service.yml

# üîÅ Verify that the service is created
kubectl get svc

# üö™ Open service in browser (Minikube only)
minikube service testpod-service

# Test
#Get minikube ip address

Test this in same local network

curl http://<mini-kube-ip>:3080/
curl http://192.168.49.2:30080/


Part	Meaning
curl	A tool to make HTTP requests from the command line. It's often used to test whether a URL is reachable and what it returns.
http://192.168.49.2	This is the IP address of the Minikube VM. It's the entry point into your Kubernetes cluster from your host machine. You found this IP using minikube ip.
:30080	This is the NodePort exposed by your Kubernetes service (testpod-service). It forwards external requests to the internal Pod‚Äôs port (8080 in your case).
/	This is the path of the URL. Since it's just a /, it hits the root endpoint of your Spring Boot app.


üëâ Note: How to delete pod and services

kubectl delete pod testpod
kubectl delete svc testpod-service

kubectl apply -f pod-01.yml
kubectl apply -f service-01.yml

###########################
‚ùÑÔ∏è Stop complete minikube
###########################
-> minikube stop
-> minikube delete
-> minikube status

#################################
‚ùÑÔ∏è To see all resources running
#################################
-> kubectl get all

################################
‚ùÑÔ∏è To delete all resources use
################################
-> kubectl delete all --all

#######################################
‚ùÑÔ∏è What are name spaces in k8s?
#######################################
-> They help logically group and isolate resources. Just like how we create folders to isolate our work in computers.
------------------
Example:
-----------------
database-ns = all database-related stuff
backend-ns = for backend applications

‚û§ Note: If we donot specifiy name space they k8S will automatically provide default name - space

##############
‚ùÑÔ∏è Commands
##############
‚û§ list all name spaces
----------------------
-> kubectl get ns

‚û§ list all pods in given name space
-----------------------------------
-> kubectl get pod -n <name-space>
######################################
‚ùÑÔ∏è How to create name space in k8s
######################################
1. Using kubectl command-
kubectl create namespace backend-ns

2.using manifest yml file 

---
apiVersion: v1
kind: Namespace
metadata:
 name: backend-ns
... 

# execute manifest yml
kubeclt apply -f <yml-file-name>

# get all resources belongs to backend-ns namespace
kubectl get all -n backend-ns

#get all pods in kube-system
kubectly get pods -n kube-system

#get all worker nodes
kubectl get nodes

#delete name space - All resource related to that will be deleted
kubectl delete ns backend-ns

#Open tunnel
minikube service <service-name>

######################################################
‚ùÑÔ∏è Namespace with POD with Service creation yml file
######################################################
---
apiVersion: v1
kind: Namespace
metadata:
 name: backend-ns
---
apiVersion: v1
kind: Pod
metadata:
  name: testpod
  namespace: backend-ns
  labels:
    app: dempapp
spec:
  containers:
    - name: test
      image: psait/pankajsiracademy:latest
      ports:
        - containerPort: 9090
---
apiVersion: v1
kind: Service
metadata:
  name: testpod-service
  namespace: backend-ns
spec:
  type: NodePort
  selector:
    app: dempapp           # This must match the Pod's label
  ports:
    - port: 80             # Exposed port for external access
      targetPort: 9090     # Port on which the app is running inside the container
      nodePort: 30080      # External port exposed on each node
...


##################
‚ùÑÔ∏è k8S Resources
##################
‚û§ When you create a Pod directly using kind: Pod, Kubernetes does not manage its lifecycle ‚Äî if it crashes or is deleted, it's gone forever unless recreated manually.
‚û§ K8S resources manages POD lifecycle
‚û§  To let Kubernetes manage, restart, and scale Pods, we use higher-level controllers like the ones you listed.

üîÅ 1) ReplicationController (RC)
üîÅ 2) ReplicaSet (RS)
üöÄ 3) Deployment
üõ∞ 4) DaemonSet
üíæ 5) StatefulSet

üì¶ What is ReplicationController (RC)?
***************************************
‚û§ A Kubernetes resource used to manage the lifecycle of Pods.
‚û§ Ensures a specified number of Pods are always running.
‚û§ Provides self-healing ‚Äî if a Pod crashes or is deleted, RC will recreate it.

manifest yml file
---
apiVersion: v1
kind: ReplicationController
metadata:
 name: webapp
spec:
 replicas: 3
 selector:
  app: dempapp
 template:
  metadata:
   name: testpod
   labels: 
    app: dempapp
  spec:
   containers:
    - name: webappcontainer
      image: psait/pankajsiracademy:latest
      ports:
      - containerPort: 9090
...

kubectl apply -f rc.yml

kubectl get all

kubectl get pods

kubectl delete pod <pod-name>

kubectl get pods

kubectl scale rc dempapp --replicas=5

kubectl scale rc dempapp --replicas=1  Explain in short

‚úÖ ReplicaSet in Kubernetes
=============================
üîπ What is a ReplicaSet?
**************************
‚û§ A ReplicaSet (RS) is a Kubernetes resource that ensures a specified number of identical Pods are running at any given time.

üí° Key Features:
******************
‚û§ Self-healing: If a Pod crashes or is manually deleted, the RS will automatically create a new Pod to maintain the desired number.

‚û§ Scaling: You can increase or decrease the number of replicas (Pods) easily.

‚û§ Selector-based matching: RS manages only those Pods that match its label selector.

Example: 
---------
‚ùÑÔ∏è Manually created Pod with label app: myapp 
************************************************
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: myappcontainer
      image: nginx
      ports:
        - containerPort: 80

---

‚ùÑÔ∏è Manually created Pod with label app: dempapp 
*************************************************
apiVersion: v1
kind: Pod
metadata:
  name: dempapp-pod
  labels:
    app: dempapp
spec:
  containers:
    - name: dempappcontainer
      image: nginx
      ports:
        - containerPort: 80

---

‚ùÑÔ∏è ReplicaSet that manages both app: dempapp and app: myapp 
*************************************************************
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-webapp
spec:
  replicas: 2
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - dempapp
          - myapp
  template:
    metadata:
      labels:
        app: dempapp   # Pods created by RS will have this label
    spec:
      containers:
        - name: webappcontainer
          image: nginx
          ports:
            - containerPort: 80

-----------------------------------
‚ùÑÔ∏è replica-set.yml for practicals
-----------------------------------

---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dempapp         # Must match pod template labels
  template:
    metadata:
      labels:
        app: dempapp
    spec:
      containers:
        - name: webappcontainer
          image: psait/pankajsiracademy:latest
          ports:
            - containerPort: 9090

---
apiVersion: v1
kind: Service
metadata:
  name: webappservice
spec:
  type: NodePort
  selector:
    app: dempapp           # Must match pod labels
  ports:
    - port: 80
      targetPort: 9090
      nodePort: 30095

#############################
‚ùÑÔ∏è Deployment in kubernetes 
#############################
+-----------------------+---------------------------+---------------------------+
| Feature               | ReplicaSet                | Deployment                |
+-----------------------+---------------------------+---------------------------+
| Manages Pods          | ‚úÖ Yes                    | ‚úÖ Yes (via ReplicaSet)   |
+-----------------------+---------------------------+---------------------------+
| Rolling Updates       | ‚ùå No                     | ‚úÖ Yes                    |
+-----------------------+---------------------------+---------------------------+
| Rollbacks             | ‚ùå No                     | ‚úÖ Yes                    |
+-----------------------+---------------------------+---------------------------+
| YAML Kind             | ReplicaSet                | Deployment                |
+-----------------------+---------------------------+---------------------------+
| Use in Real World     | Rare                      | Very Common               |
+-----------------------+---------------------------+---------------------------+

‚û§ A deployment in kubernetes is one of the most recommended and used resource for managing POD lifecycle. It ensures reliable application deployment with features like zero downtime ,auto-scaling ,rolling updates and rollback capabilities .

üìÖ Key advantages of using deployments 
***************************************
üëâ Zero Downtime: Deployments ensure high availability by using strategies like rolling updates . Even when pods are being updated, the service remains available to users .

üëâ Auto Scaling: With Kubernetes Horizontal Pod Autoscaler, you can automatically scale your Pods based on CPU or memory usage (or other custom metrics).

üëâ Rolling Update & Rollback: Kubernetes allows rolling updates, which means it will gradually update Pods one by one, ensuring the application remains available throughout the process. If something goes wrong during the update, you can rollback to a previous stable version.

‚ú® When to choose which strategy ?
====================================
üëâ Use Rolling Update : 
------------------------
‚û§ For most production workloads where high availability and zero downtime are required 
‚û§ When you are gradually releasing new versions of your application and want to avoid service disruptions 

üëâ Use Recreate : 
--------------------
‚û§ For non critical applications or during maintainence windows where it's okay to have temporary outage.
‚û§ When you need to clear everything and redeploy fresh pods(Eg: clearing persistent state or major upgrades).

############################
‚ùÑÔ∏è deployment-service.yml
############################
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dempapp         # Must match pod template labels
  template:
    metadata:
      labels:
        app: dempapp
    spec:
      containers:
        - name: webappcontainer
          image: psait/pankajsiracademy:latest
          ports:
            - containerPort: 9090
---
apiVersion: v1
kind: Service
metadata:
  name: webappservice
spec:
  type: NodePort
  selector:
    app: dempapp           # Must match pod labels
  ports:
    - port: 80
      targetPort: 9090
      nodePort: 30095

##########################################################################

üîß 1. Deployment
******************
‚û§ apiVersion: apps/v1
Uses the apps/v1 API to define a Deployment.

‚û§ kind: Deployment
Declares that this is a Deployment (used to manage pods).

‚û§ metadata:
  name: webapp
Assigns the Deployment a name: webapp.

‚û§ spec:
  replicas: 3
Tells Kubernetes to run 3 replicas (3 pods) of your app.

  selector:
    matchLabels:
      app: dempapp
The deployment will manage pods that have the label app: dempapp.

  template:
    metadata:
      labels:
        app: dempapp

‚û§ This is the pod template. Every pod created will be labeled app: dempapp.
‚û§ This must match the selector above.

    spec:
      containers:
        - name: webappcontainer
          image: psait/pankajsiracademy:latest
          ports:
            - containerPort: 9090

‚û§ This is the container spec inside the pod:

name: webappcontainer
image: Docker image hosted at Docker Hub (psait/pankajsiracademy:latest)
containerPort: The app runs on port 9090 inside the container.

üåê 2. Service
****************
‚û§ apiVersion: v1
Uses the core v1 API to define a Service.

‚û§ kind: Service
Declares a Service (used to expose pods).

‚û§ metadata:
  name: webappservice
The service name is webappservice.

spec:
  type: NodePort
NodePort type: Exposes your app externally by opening a port on every node in the cluster.

‚û§ Clients can access it via:
http://<NodeIP>:30095

  selector:
    app: dempapp

‚û§ The service targets pods with label app: dempapp ‚Äî which matches the Deployment.


  ports:
    - port: 80
      targetPort: 9090
      nodePort: 30095

‚û§ port: 80: The port the service receives traffic on (internally).
‚û§ targetPort: 9090: Forwards traffic to container's port 9090.
‚û§ nodePort: 30095: Opens this port on the Kubernetes node ‚Äî used to access the service from outside the cluster.

##################################
‚ùÑÔ∏è Deployment with load balancer
###################################
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dempapp         # Must match pod template labels
  template:
    metadata:
      labels:
        app: dempapp
    spec:
      containers:
        - name: webappcontainer
          image: psait/pankajsiracademy:latest
          ports:
            - containerPort: 9090

---
apiVersion: v1
kind: Service
metadata:
  name: webappservice
spec:
  type: LoadBalancer     # ‚úÖ Changed from NodePort to LoadBalancer
  selector:
    app: dempapp
  ports:
    - port: 80
      targetPort: 9090

üëâ Note : 
-----------
‚û§ First : Is each worker node a separate VM?
‚úÖ Yes, each worker node in EKS is an individual EC2 instance (VM).

-> So if you have 4 worker nodes EKS will launch 4 separate EC2 instances in your account (under the hood).

#########################
‚ùÑÔ∏è What is Amazon EKS ?
#########################
‚û§ Amazon Elastic Kubernetes Service is a managed service that makes it easy for you to run kubernetes on AWS without needing to install and operate your own kubernetes clusters.

üìÖ EKS setup 
**************
üëâ Step 1: Create EKS management host in AWS 
----------------------------------------------
‚û§ Launch new ubuntu VM using AWS EC2(t2.micro)
‚û§ Connect to machine and install kubectl using below commands 

curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin
kubectl version --short --client

‚û§ Install AWS CLI latest version using below commands 

sudo apt install unzip
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws --version

‚û§ Install eksctl using below commands 

curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
eksctl version

üëâ Step 2: Create IAM role and attach to EKS management host 
--------------------------------------------------------------
‚û§ Create new role using IAM service (Select usecase-EC2)
‚û§ Add below permission for the role 
  => Administrator-acces 
‚û§ Enter role name(eksrole)
‚û§ Attach created role to EKS management host (Select EC2 => click on security => Modify IAM role => attach IAM role we have created)

üëâ Step 3: Create EKS cluster using ekctl 
--------------------------------------------
Syntax: 
eksctl create cluster --name cluster-name
--region region-name
--node-type instance-type
--nodes-min 2
--nodes-max 2 \ --zones ,

-> Mumbai 
===========
eksctl create cluster --name psait-cluster4 --region ap-south-1 --node-type t3.micro  --zones ap-south-1a,ap-south-1b

-> If we dont specify the number of worker nodes then by default it will create 2 worker nodes.

‚û§ After the cluster is created we can check nodes using the below command 
kubectl get nodes

üìÖ How to create more worker nodes in EKS ?
*********************************************
‚û§ There are two ways to create more worker nodes: 

‚úÖ Option 1: CLI method (Add --nodes flag)
============================================
‚û§ You can specify the number of nodes when creating the cluster using the --nodes flag: 

eksctl create cluster \
  --name psait-cluster4 \
  --region ap-south-1 \
  --node-type t2.medium \
  --zones ap-south-1a,ap-south-1b \
  --nodes 4 \
  --nodes-min 2 \
  --nodes-max 6

üåü What this means : 
----------------------
--nodes 4 -> Start with 4 worker nodes 

--nodes-min 2 -> Minimum nodes for autoscaling 

--nodes-max 6 -> Max nodes for autoscaling

‚û§ Nodes will be spread across the AZs you mention (Load balancer). 

‚úÖ Option 2: YAML config file (More Flexible)
===============================================
‚û§ Create a cluster.yml like this: 

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: psait-cluster4
  region: ap-south-1

availabilityZones: ["ap-south-1a", "ap-south-1b"]

nodeGroups:
  - name: ng-1
    instanceType: t2.medium
    desiredCapacity: 4
    minSize: 2
    maxSize: 6
    volumeSize: 20
  
üëâ Note: We can use terraform for the same to create EKS setup.

‚ú® YAML file to deploy application in EKS with Load Balancer 
**************************************************************
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: webapp
spec:
 replicas: 2
 strategy: 
  type: RollingUpdate
 selector:
  matchLabels:
   app: javawebapp
 template:
  metadata:
   name: javawebpod
   labels:
    app: javawebapp
  spec:
   containers:
   - name: webappcontainer
     image: psait/pankajsiracademy:latest
     ports:
     - containerPort: 9090
---
apiVersion: v1
kind: Service
metadata:
 name: websvc
spec:
 type: LoadBalancer
 selector:
  app: javawebapp
 ports:
  - port: 80
    targetPort: 9090
...

üìÖ pipeline - k8s + maven + docker + jenkins
----------------------------------------------
pipeline {
    agent any
    
    tools{
        maven "maven-3.9.9"
    }

    stages {
        stage('Clone Repo') {
            steps {
                git branch: 'main', url: 'https://github.com/pankajmutha14/docker-test.git'
            }
        }
        stage('Maven Build') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('Docker Image') {
            steps {
                sh 'docker build -t psait/pankajsiracademy:latest .'
            }
        }
        stage('k8s deployment') {
            steps {
                sh 'kubectl apply -f k8s-deploy.yml'
            }
        }
    }
}

##############################
‚ùÑÔ∏è Scaling in Kubernetes
#############################

1. HPA - Horizontal Pod Autoscaler
************************************
üëâ What it does: Adds or removes pods based on CPU/memory usage, custom metrics, or external metrics.
‚û§ Use case: When traffic increases, Kubernetes spins up more pods to handle the load.
‚û§ Controlled by: The HorizontalPodAutoscaler object.

Example: If CPU usage goes above 80%, HPA can increase the pod count from 2 to 5 automatically

2. VPA - Vertical Pod Autoscaler
*********************************
üëâ What it does: Adjusts CPU and memory requests/limits of a pod automatically.

‚û§ Apply Load in HPA
-----------------------
kubectl run -i --tty load-generator --rm \
  --image=busybox --restart=Never \
  -- /bin/sh -c "while true; do wget -q -O- http://hpa-demo-deployment; sleep 0.01; done"


kubectl get hpa -w
kubectl describe deploy hpa-demo-deployment
kubectl get hpa
kubectl get events
